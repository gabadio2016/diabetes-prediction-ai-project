{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Baseline Testing"
      ],
      "metadata": {
        "id": "XAWHVOG78Aqq"
      },
      "id": "XAWHVOG78Aqq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports for metrics and plots**"
      ],
      "metadata": {
        "id": "KPTT6Jda7_tD"
      },
      "id": "KPTT6Jda7_tD"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score, roc_curve\n",
        ")\n"
      ],
      "metadata": {
        "id": "SWj_CUaG7748"
      },
      "id": "SWj_CUaG7748",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric function definitions**"
      ],
      "metadata": {
        "id": "KKx7_CAt8Gmj"
      },
      "id": "KKx7_CAt8Gmj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the functions that compute the metrics and plot confusion matrices\n",
        "def evaluate_classification(y_true, y_pred, y_proba=None):\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1 Score': f1_score(y_true, y_pred)\n",
        "    }\n",
        "    if y_proba is not None:\n",
        "        metrics['ROC-AUC'] = roc_auc_score(y_true, y_proba)\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "HqyNPm-k8D0z"
      },
      "id": "HqyNPm-k8D0z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix visualization**"
      ],
      "metadata": {
        "id": "VcgEFeJRjM98"
      },
      "id": "VcgEFeJRjM98"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\"):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rv8Z769sjL5k"
      },
      "id": "rv8Z769sjL5k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance comparison setup**"
      ],
      "metadata": {
        "id": "VxQ_pnP-FcHH"
      },
      "id": "VxQ_pnP-FcHH"
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "def compare_model_performance(model_name, y_true, y_pred, y_proba=None):\n",
        "    results[model_name] = evaluate_classification(y_true, y_pred, y_proba)\n"
      ],
      "metadata": {
        "id": "lbOn8LJ4Febc"
      },
      "id": "lbOn8LJ4Febc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline model training**"
      ],
      "metadata": {
        "id": "N6s_D8NwFics"
      },
      "id": "N6s_D8NwFics"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train baseline model\n",
        "lr = LogisticRegression(max_iter=500, random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "y_proba_lr = lr.predict_proba(X_test)[:, 1]\n"
      ],
      "metadata": {
        "id": "ne3ZvaFSSswU"
      },
      "id": "ne3ZvaFSSswU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate and visualize**"
      ],
      "metadata": {
        "id": "5kUfy0IzSzHF"
      },
      "id": "5kUfy0IzSzHF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics\n",
        "baseline_metrics = evaluate_classification(y_test, y_pred_lr, y_proba_lr)\n",
        "compare_model_performance(\"Logistic Regression (Baseline)\", y_test, y_pred_lr, y_proba_lr)\n",
        "\n",
        "# Display confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred_lr, \"Logistic Regression (Baseline)\")\n",
        "\n",
        "# Show metrics\n",
        "pd.DataFrame([baseline_metrics])\n"
      ],
      "metadata": {
        "id": "phxLf09Ijene"
      },
      "id": "phxLf09Ijene",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}